{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-22T16:49:43.210437Z",
     "iopub.status.busy": "2025-04-22T16:49:43.210153Z",
     "iopub.status.idle": "2025-04-22T16:49:58.315940Z",
     "shell.execute_reply": "2025-04-22T16:49:58.315355Z",
     "shell.execute_reply.started": "2025-04-22T16:49:43.210414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 16:49:45.534035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745340585.790380      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745340585.863013      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:49:58.317642Z",
     "iopub.status.busy": "2025-04-22T16:49:58.317134Z",
     "iopub.status.idle": "2025-04-22T16:49:58.327166Z",
     "shell.execute_reply": "2025-04-22T16:49:58.326558Z",
     "shell.execute_reply.started": "2025-04-22T16:49:58.317622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:49:58.328214Z",
     "iopub.status.busy": "2025-04-22T16:49:58.327959Z",
     "iopub.status.idle": "2025-04-22T16:50:03.270614Z",
     "shell.execute_reply": "2025-04-22T16:50:03.269873Z",
     "shell.execute_reply.started": "2025-04-22T16:49:58.328192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:03.272891Z",
     "iopub.status.busy": "2025-04-22T16:50:03.272645Z",
     "iopub.status.idle": "2025-04-22T16:50:09.001503Z",
     "shell.execute_reply": "2025-04-22T16:50:09.000928Z",
     "shell.execute_reply.started": "2025-04-22T16:50:03.272871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8180379e36374b578afe6009cb6ba992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9846835c1b20411498b7a570f4a778c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772e997979994e529fb2ed2972472c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fff46bc99bf4852bde9e15fbca708e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e829f21b5ee415fa9c86c123091dae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:09.002881Z",
     "iopub.status.busy": "2025-04-22T16:50:09.002310Z",
     "iopub.status.idle": "2025-04-22T16:50:09.007827Z",
     "shell.execute_reply": "2025-04-22T16:50:09.007100Z",
     "shell.execute_reply.started": "2025-04-22T16:50:09.002860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:09.008973Z",
     "iopub.status.busy": "2025-04-22T16:50:09.008684Z",
     "iopub.status.idle": "2025-04-22T16:50:09.056081Z",
     "shell.execute_reply": "2025-04-22T16:50:09.055338Z",
     "shell.execute_reply.started": "2025-04-22T16:50:09.008956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Shuffle and select random samples\n",
    "squad_dataset = {\n",
    "    \"train\": squad_dataset[\"train\"].shuffle(seed=42).select(range(10000)),\n",
    "    \"validation\": squad_dataset[\"validation\"].shuffle(seed=42).select(range(2000)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:09.057106Z",
     "iopub.status.busy": "2025-04-22T16:50:09.056889Z",
     "iopub.status.idle": "2025-04-22T16:50:09.061784Z",
     "shell.execute_reply": "2025-04-22T16:50:09.061176Z",
     "shell.execute_reply.started": "2025-04-22T16:50:09.057081Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 10000\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "     num_rows: 2000\n",
       " })}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:09.062577Z",
     "iopub.status.busy": "2025-04-22T16:50:09.062408Z",
     "iopub.status.idle": "2025-04-22T16:50:10.021856Z",
     "shell.execute_reply": "2025-04-22T16:50:10.020956Z",
     "shell.execute_reply.started": "2025-04-22T16:50:09.062558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max answer length (words) in train set: 29\n"
     ]
    }
   ],
   "source": [
    "# flatten all answer texts and compute max length\n",
    "max_len = max(\n",
    "    len(ans.split())\n",
    "    for example in squad_dataset[\"train\"]\n",
    "    for ans in example[\"answers\"][\"text\"]\n",
    ")\n",
    "\n",
    "print(\"Max answer length (words) in train set:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:10.022997Z",
     "iopub.status.busy": "2025-04-22T16:50:10.022731Z",
     "iopub.status.idle": "2025-04-22T16:50:10.028696Z",
     "shell.execute_reply": "2025-04-22T16:50:10.028086Z",
     "shell.execute_reply.started": "2025-04-22T16:50:10.022973Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5727e2483acd2414000deef0',\n",
       " 'title': 'Rule_of_law',\n",
       " 'context': 'One important aspect of the rule-of-law initiatives is the study and analysis of the rule of law’s impact on economic development. The rule-of-law movement cannot be fully successful in transitional and developing countries without an answer to the question: does the rule of law matter for economic development or not? Constitutional economics is the study of the compatibility of economic and financial decisions within existing constitutional law frameworks, and such a framework includes government spending on the judiciary, which, in many transitional and developing countries, is completely controlled by the executive. It is useful to distinguish between the two methods of corruption of the judiciary: corruption by the executive branch, in contrast to corruption by private actors.',\n",
       " 'question': 'In developing countries, who makes most of the spending decisions?',\n",
       " 'answers': {'text': ['the executive'], 'answer_start': [612]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:10.030859Z",
     "iopub.status.busy": "2025-04-22T16:50:10.030639Z",
     "iopub.status.idle": "2025-04-22T16:50:13.442105Z",
     "shell.execute_reply": "2025-04-22T16:50:13.441313Z",
     "shell.execute_reply.started": "2025-04-22T16:50:10.030842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) EXTRACT RAW TEXT LISTS\n",
    "# ------------------------------------------------\n",
    "def extract_texts(split):\n",
    "    contexts  = [ex[\"context\"]  for ex in split]\n",
    "    questions = [ex[\"question\"] for ex in split]\n",
    "    answers    = [ex[\"answers\"][\"text\"][0] for ex in split]  # first (gold) answer\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_ctxs, train_qs, train_as  = extract_texts(squad_dataset[\"train\"])\n",
    "val_ctxs,   val_qs,   val_as    = extract_texts(squad_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:13.443204Z",
     "iopub.status.busy": "2025-04-22T16:50:13.442945Z",
     "iopub.status.idle": "2025-04-22T16:50:13.451257Z",
     "shell.execute_reply": "2025-04-22T16:50:13.450592Z",
     "shell.execute_reply.started": "2025-04-22T16:50:13.443182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3) PREPARE DECODER INPUT/OUTPUT WITH <start>/<end> TOKENS\n",
    "# ------------------------------------------------\n",
    "train_in  = [f\"<start> {a}\" for a in train_as]\n",
    "train_out = [f\"{a} <end>\" for a in train_as]\n",
    "val_in    = [f\"<start> {a}\" for a in val_as]\n",
    "val_out   = [f\"{a} <end>\" for a in val_as]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:13.452236Z",
     "iopub.status.busy": "2025-04-22T16:50:13.452015Z",
     "iopub.status.idle": "2025-04-22T16:50:14.890070Z",
     "shell.execute_reply": "2025-04-22T16:50:14.889478Z",
     "shell.execute_reply.started": "2025-04-22T16:50:13.452214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4) TOKENIZER SETUP\n",
    "# ------------------------------------------------\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "all_texts = train_ctxs + train_qs + train_in + train_out \\\n",
    "          + val_ctxs   + val_qs   + val_in   + val_out\n",
    "\n",
    "VOCAB_SIZE  = 20000\n",
    "custom_filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'  # note: removed '<' and '>'\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=VOCAB_SIZE,\n",
    "    oov_token=\"<UNK>\",\n",
    "    filters=custom_filters\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:14.891071Z",
     "iopub.status.busy": "2025-04-22T16:50:14.890836Z",
     "iopub.status.idle": "2025-04-22T16:50:14.895765Z",
     "shell.execute_reply": "2025-04-22T16:50:14.895045Z",
     "shell.execute_reply.started": "2025-04-22T16:50:14.891054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[\"<start>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:14.897011Z",
     "iopub.status.busy": "2025-04-22T16:50:14.896567Z",
     "iopub.status.idle": "2025-04-22T16:50:14.907869Z",
     "shell.execute_reply": "2025-04-22T16:50:14.907274Z",
     "shell.execute_reply.started": "2025-04-22T16:50:14.896972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[\"<end>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:14.908674Z",
     "iopub.status.busy": "2025-04-22T16:50:14.908513Z",
     "iopub.status.idle": "2025-04-22T16:50:16.044280Z",
     "shell.execute_reply": "2025-04-22T16:50:16.043743Z",
     "shell.execute_reply.started": "2025-04-22T16:50:14.908662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5) ENCODE & PAD SEQUENCES\n",
    "\n",
    "def encode_and_pad(texts, maxlen):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "MAX_CTX_LEN = 300\n",
    "MAX_Q_LEN   = 30\n",
    "MAX_ANS_LEN = 30\n",
    "\n",
    "X_ctx_train = encode_and_pad(train_ctxs, MAX_CTX_LEN)\n",
    "X_q_train   = encode_and_pad(train_qs,   MAX_Q_LEN)\n",
    "X_ans_in_tr = encode_and_pad(train_in,    MAX_ANS_LEN)\n",
    "y_ans_tr    = encode_and_pad(train_out,   MAX_ANS_LEN)\n",
    "\n",
    "X_ctx_val   = encode_and_pad(val_ctxs,    MAX_CTX_LEN)\n",
    "X_q_val     = encode_and_pad(val_qs,      MAX_Q_LEN)\n",
    "X_ans_in_val= encode_and_pad(val_in,      MAX_ANS_LEN)\n",
    "y_ans_val   = encode_and_pad(val_out,     MAX_ANS_LEN)\n",
    "\n",
    "# expand last dim for sparse loss\n",
    "y_ans_train = y_ans_tr.reshape(*y_ans_tr.shape, 1)\n",
    "y_ans_valid = y_ans_val.reshape(*y_ans_val.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:16.045110Z",
     "iopub.status.busy": "2025-04-22T16:50:16.044933Z",
     "iopub.status.idle": "2025-04-22T16:50:16.050612Z",
     "shell.execute_reply": "2025-04-22T16:50:16.049834Z",
     "shell.execute_reply.started": "2025-04-22T16:50:16.045096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  12, 8078,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ans_in_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Encoder-Decoder, No Attention Mechanism)\n",
    "### Not used as the accuracy was very low < 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6) BUILD ENCODER–DECODER\n",
    "# ------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Dense, Concatenate, AdditiveAttention\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EMBED_DIM = 128\n",
    "ENC_UNITS = 64\n",
    "DEC_UNITS = 64\n",
    "\n",
    "# -- Context Encoder\n",
    "ctx_in  = Input(shape=(MAX_CTX_LEN,), name=\"context_input\")\n",
    "ctx_emb = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(ctx_in)\n",
    "_, st_h_c, st_c_c = LSTM(ENC_UNITS, return_state=True, name=\"ctx_encoder\")(ctx_emb)\n",
    "\n",
    "# -- Question Encoder\n",
    "q_in   = Input(shape=(MAX_Q_LEN,), name=\"question_input\")\n",
    "q_emb  = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(q_in)\n",
    "_, st_h_q, st_c_q = LSTM(ENC_UNITS, return_state=True, name=\"q_encoder\")(q_emb)\n",
    "\n",
    "# -- Merge states\n",
    "merged_h = Concatenate()([st_h_c, st_h_q])\n",
    "merged_c = Concatenate()([st_c_c, st_c_q])\n",
    "proj_h   = Dense(DEC_UNITS, activation=\"tanh\")(merged_h)\n",
    "proj_c   = Dense(DEC_UNITS, activation=\"tanh\")(merged_c)\n",
    "\n",
    "# -- Decoder\n",
    "ans_in   = Input(shape=(MAX_ANS_LEN,), name=\"answer_input\")\n",
    "ans_emb  = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(ans_in)\n",
    "dec_lstm = LSTM(DEC_UNITS, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "dec_outs, _, _ = dec_lstm(ans_emb, initial_state=[proj_h, proj_c])\n",
    "dec_dense = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"decoder_output\")\n",
    "outputs   = dec_dense(dec_outs)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[ctx_in, q_in, ans_in],\n",
    "    outputs=outputs\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7) TRAIN\n",
    "# ------------------------------------------------\n",
    "model.fit(\n",
    "    x   = [X_ctx_train, X_q_train, X_ans_in_tr],\n",
    "    y   = y_ans_train,\n",
    "    batch_size    = 64,\n",
    "    epochs        = 10,\n",
    "    validation_data = ([X_ctx_val, X_q_val, X_ans_in_val], y_ans_valid)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Encoder-Decoder, With Attention Mechanism)\n",
    "### Not used as the accuracy was very low < 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Dense, Concatenate, AdditiveAttention\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ─── HYPERPARAMETERS ──────────────────────────────────────────────\n",
    "EMBED_DIM     = 128\n",
    "ENC_UNITS     = 256\n",
    "DEC_UNITS     = 256\n",
    "MAX_CTX_LEN   = 300\n",
    "MAX_Q_LEN     = 30\n",
    "MAX_ANS_LEN   = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# ─── INPUTS & EMBEDDINGS ──────────────────────────────────────────\n",
    "ctx_input = Input(shape=(MAX_CTX_LEN,), name=\"context_input\")\n",
    "q_input   = Input(shape=(MAX_Q_LEN,),   name=\"question_input\")\n",
    "ans_input = Input(shape=(MAX_ANS_LEN,), name=\"answer_input\")\n",
    "\n",
    "# notice mask_zero=False → no masking\n",
    "ctx_emb = Embedding(VOCAB_SIZE, EMBED_DIM, name=\"ctx_embedding\")(ctx_input)\n",
    "q_emb   = Embedding(VOCAB_SIZE, EMBED_DIM, name=\"q_embedding\")(q_input)\n",
    "ans_emb = Embedding(VOCAB_SIZE, EMBED_DIM, name=\"ans_embedding\")(ans_input)\n",
    "\n",
    "# ─── ENCODERS ─────────────────────────────────────────────────────\n",
    "# Context encoder returns full sequence + final states\n",
    "ctx_seq, ctx_h, ctx_c = LSTM(\n",
    "    ENC_UNITS, return_sequences=True, return_state=True, name=\"ctx_encoder\"\n",
    ")(ctx_emb)\n",
    "\n",
    "# Question encoder returns only final states\n",
    "_, q_h, q_c = LSTM(\n",
    "    ENC_UNITS, return_sequences=False, return_state=True, name=\"q_encoder\"\n",
    ")(q_emb)\n",
    "\n",
    "# ─── STATE FUSION ────────────────────────────────────────────────\n",
    "merged_h = Concatenate(name=\"merge_h\")([ctx_h, q_h])\n",
    "merged_c = Concatenate(name=\"merge_c\")([ctx_c, q_c])\n",
    "proj_h   = Dense(DEC_UNITS, activation=\"tanh\", name=\"proj_h\")(merged_h)\n",
    "proj_c   = Dense(DEC_UNITS, activation=\"tanh\", name=\"proj_c\")(merged_c)\n",
    "\n",
    "# ─── DECODER + ADDITIVE ATTENTION ─────────────────────────────────\n",
    "dec_seq, _, _ = LSTM(\n",
    "    DEC_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder_lstm\"\n",
    ")(ans_emb, initial_state=[proj_h, proj_c])\n",
    "\n",
    "# use AdditiveAttention instead of Attention\n",
    "attn_out = AdditiveAttention(name=\"attention\")([dec_seq, ctx_seq])\n",
    "\n",
    "dec_concat = Concatenate(name=\"decoder_concat\")([dec_seq, attn_out])\n",
    "decoder_outputs = Dense(\n",
    "    VOCAB_SIZE, activation=\"softmax\", name=\"decoder_output\"\n",
    ")(dec_concat)\n",
    "\n",
    "# ─── MODEL & COMPILATION ────────────────────────────────────────\n",
    "model = Model(\n",
    "    inputs=[ctx_input, q_input, ans_input],\n",
    "    outputs=decoder_outputs,\n",
    "    name=\"QA_EncoderDecoder_with_AdditiveAttention\"\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7) TRAINING THE MODEL\n",
    "\n",
    "mask = (y_ans_train != 0).astype(\"float32\")  # 1 for real tokens, 0 for padding\n",
    "\n",
    "history = model.fit(\n",
    "    x=[X_ctx_train, X_q_train, X_ans_in_tr],\n",
    "    y=y_ans_train,\n",
    "    sample_weight=mask,         # masks out padding positions\n",
    "    batch_size=64,\n",
    "    epochs=12,\n",
    "    validation_data=(\n",
    "        [X_ctx_val, X_q_val, X_ans_in_val],\n",
    "        y_ans_valid\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assumes you have in scope:\n",
    "#  - model                    : your trained encoder–decoder with attention\n",
    "#  - tokenizer                : the fitted Tokenizer\n",
    "#  - MAX_CTX_LEN, MAX_Q_LEN   : max lengths for context/question\n",
    "#  - MAX_ANS_LEN              : max length for generated answer\n",
    "\n",
    "def generate_answer(model, tokenizer, context, question,\n",
    "                    max_ctx_len=MAX_CTX_LEN,\n",
    "                    max_q_len=MAX_Q_LEN,\n",
    "                    max_ans_len=MAX_ANS_LEN):\n",
    "    # 1) Encode & pad context + question\n",
    "    ctx_seq = tokenizer.texts_to_sequences([context])\n",
    "    ctx_seq = pad_sequences(ctx_seq, maxlen=max_ctx_len, padding=\"post\")\n",
    "\n",
    "    q_seq   = tokenizer.texts_to_sequences([question])\n",
    "    q_seq   = pad_sequences(q_seq,   maxlen=max_q_len,   padding=\"post\")\n",
    "\n",
    "    # 2) Prepare empty decoder input, seed with <start> token\n",
    "    start_idx = tokenizer.word_index[\"<start>\"]\n",
    "    end_idx   = tokenizer.word_index[\"<end>\"]\n",
    "    ans_seq = np.zeros((1, max_ans_len), dtype=\"int32\")\n",
    "    ans_seq[0, 0] = start_idx\n",
    "\n",
    "    # 3) Iteratively predict next token\n",
    "    for i in range(1, max_ans_len):\n",
    "        preds = model.predict([ctx_seq, q_seq, ans_seq], verbose=0)\n",
    "        next_token = np.argmax(preds[0, i-1])\n",
    "        print(f\"Step {i}, next_token id = {next_token}\")\n",
    "        ans_seq[0, i] = next_token\n",
    "        if next_token == end_idx:\n",
    "            print(\"Hit <end>, stopping generation\")\n",
    "            break\n",
    "\n",
    "\n",
    "    # 4) Convert token IDs back to words\n",
    "    index_to_word = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    # skip padding (0) and <start>, stop at <end>\n",
    "    output_tokens = []\n",
    "    for idx in ans_seq[0]:\n",
    "        if idx == 0 or idx == start_idx:\n",
    "            continue\n",
    "        if idx == end_idx:\n",
    "            break\n",
    "        output_tokens.append(index_to_word.get(idx, \"\"))\n",
    "\n",
    "    return \" \".join(output_tokens)\n",
    "\n",
    "# ── Example Usage ───────────────────────────────────────────────────\n",
    "context  = squad_dataset[\"validation\"][2][\"context\"]\n",
    "question = squad_dataset[\"validation\"][2][\"question\"]\n",
    "ground_truth_answer = squad_dataset[\"validation\"][2][\"answers\"][\"text\"][0]\n",
    "\n",
    "answer = generate_answer(model, tokenizer, context, question)\n",
    "print(\"Context: \", context)\n",
    "print(\"Question:\", question)\n",
    "print(\"Model Answer:  \", answer)\n",
    "print(\"Ground Truth Answer:  \", ground_truth_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Dense, LayerNormalization, Dropout,\n",
    "    MultiHeadAttention, Lambda, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:50:16.051505Z",
     "iopub.status.busy": "2025-04-22T16:50:16.051238Z",
     "iopub.status.idle": "2025-04-22T16:50:16.063314Z",
     "shell.execute_reply": "2025-04-22T16:50:16.062559Z",
     "shell.execute_reply.started": "2025-04-22T16:50:16.051487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define temperature scaling layer\n",
    "class TemperatureScaling(tf.keras.layers.Layer):\n",
    "    def __init__(self, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, logits):\n",
    "        return logits / self.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define masked loss and accuracy metrics\n",
    "def masked_loss(y_true, y_pred, label_smoothing=0.1):\n",
    "    # Squeeze the last dimension of y_true to match y_pred shape\n",
    "    y_true = tf.squeeze(y_true, axis=-1)\n",
    "\n",
    "    # Create mask to ignore padding tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "\n",
    "    # Convert y_true to integer type for one_hot encoding\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Convert y_true to one-hot with label smoothing\n",
    "    num_classes = tf.shape(y_pred)[-1]\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes, dtype=y_pred.dtype)\n",
    "\n",
    "    # Apply label smoothing\n",
    "    y_true = y_true * (1.0 - label_smoothing) + (\n",
    "        label_smoothing / tf.cast(num_classes, y_pred.dtype)\n",
    "    )\n",
    "\n",
    "    # Calculate cross entropy loss\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "\n",
    "    # Apply mask and return mean loss over non-padding tokens\n",
    "    loss = loss * mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    # Squeeze the last dimension of y_true to match y_pred shape\n",
    "    y_true = tf.squeeze(y_true, axis=-1)\n",
    "\n",
    "    # Create mask to ignore padding tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "\n",
    "    # Get predicted tokens\n",
    "    predictions = tf.argmax(y_pred, axis=-1)\n",
    "    predictions = tf.cast(predictions, tf.int32)\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Calculate accuracy only on non-padding tokens\n",
    "    accuracies = tf.equal(y_true, predictions)\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    # Convert to float and take mean\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:51:46.457822Z",
     "iopub.status.busy": "2025-04-22T16:51:46.457517Z",
     "iopub.status.idle": "2025-04-22T16:51:46.952467Z",
     "shell.execute_reply": "2025-04-22T16:51:46.951797Z",
     "shell.execute_reply.started": "2025-04-22T16:51:46.457800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Transformer_QA_Seq2Seq\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Transformer_QA_Seq2Seq\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ context_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ question_input            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ answer_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ src_concat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                           │                        │                │ question_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ token_embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ src_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │ answer_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ pos_enc_src               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ enc_padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ src_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_9    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ pos_enc_src[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ pos_enc_src[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                           │                        │                │ enc_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pos_enc_src[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                           │                        │                │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_10   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ layer_normalization_1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
       "│                           │                        │                │ enc_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_18    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_11   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ layer_normalization_1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
       "│                           │                        │                │ enc_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_19    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ pos_enc_tgt               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncoding</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ look_ahead_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ answer_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_12   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ pos_enc_tgt[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ pos_enc_tgt[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                           │                        │                │ look_ahead_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pos_enc_tgt[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                           │                        │                │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_21    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_20    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_13   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ layer_normalization_2… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ enc_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_22    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_23    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_14   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ layer_normalization_2… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ look_ahead_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_24    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_15   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ layer_normalization_2… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ enc_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_25    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_26    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_16   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ layer_normalization_2… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ look_ahead_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_27    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_17   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ layer_normalization_2… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ enc_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_28    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_29    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,000</span> │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ temperature_scaling_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TemperatureScaling</span>)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ temperature_scaling_1… │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ context_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ question_input            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ answer_input (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ src_concat (\u001b[38;5;33mConcatenate\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ context_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                           │                        │                │ question_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ token_embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │      \u001b[38;5;34m2,560,000\u001b[0m │ src_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │ answer_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ pos_enc_src               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ enc_padding_mask (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m330\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ src_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_9    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m527,488\u001b[0m │ pos_enc_src[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ pos_enc_src[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                           │                        │                │ enc_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_15 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ pos_enc_src[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                           │                        │                │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_16 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_10   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m527,488\u001b[0m │ layer_normalization_1… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
       "│                           │                        │                │ enc_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_17 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_18 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_18    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_11   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m527,488\u001b[0m │ layer_normalization_1… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
       "│                           │                        │                │ enc_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_19 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_19    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ pos_enc_tgt               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ token_embedding[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mPositionalEncoding\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ look_ahead_mask (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ answer_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_1… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_12   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m527,488\u001b[0m │ pos_enc_tgt[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ pos_enc_tgt[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                           │                        │                │ look_ahead_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m65,664\u001b[0m │ dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_21 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ pos_enc_tgt[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                           │                        │                │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_20 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_1… │\n",
       "│                           │                        │                │ dropout_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_21    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_20    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m330\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_13   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m527,488\u001b[0m │ layer_normalization_2… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ enc_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_22 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_22    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_23 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_23    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_14   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m527,488\u001b[0m │ layer_normalization_2… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ look_ahead_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_39 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_24 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_24    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_15   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m527,488\u001b[0m │ layer_normalization_2… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ enc_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_41 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_25 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_25    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_42 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_26 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_26    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_16   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m527,488\u001b[0m │ layer_normalization_2… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ look_ahead_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_44 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_27 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_27    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ multi_head_attention_17   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m527,488\u001b[0m │ layer_normalization_2… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ layer_normalization_2… │\n",
       "│                           │                        │                │ enc_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_46 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_28 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_28    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m66,048\u001b[0m │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m65,664\u001b[0m │ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_47 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ add_29 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_2… │\n",
       "│                           │                        │                │ dropout_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ layer_normalization_29    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │            \u001b[38;5;34m256\u001b[0m │ add_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_output (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m20000\u001b[0m)      │      \u001b[38;5;34m2,580,000\u001b[0m │ layer_normalization_2… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ temperature_scaling_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m20000\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ decoder_output[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mTemperatureScaling\u001b[0m)      │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m20000\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ temperature_scaling_1… │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,681,504</span> (40.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,681,504\u001b[0m (40.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,681,504</span> (40.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,681,504\u001b[0m (40.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── HYPERPARAMETERS ──────────────────────────────────────────────\n",
    "D_MODEL       = 128\n",
    "NUM_LAYERS    = 3\n",
    "NUM_HEADS     = 8\n",
    "DFF           = 512\n",
    "DROPOUT_RATE  = 0.1\n",
    "MAX_CTX_LEN   = 300\n",
    "MAX_Q_LEN     = 30\n",
    "MAX_SRC_LEN   = MAX_CTX_LEN + MAX_Q_LEN\n",
    "MAX_ANS_LEN   = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "CLIP_NORM = 1.0 # Add gradient clipping to prevent exploding gradients\n",
    "\n",
    "# ─── POSITIONAL ENCODING ───────────────────────────────────────────\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        pos = np.arange(maxlen)[:, None]\n",
    "        i   = np.arange(d_model)[None, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        angle_rads  = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[None, ...], tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# ─── TRANSFORMER LAYERS ────────────────────────────────────────────\n",
    "def transformer_encoder_layer(x, *, mask=None):\n",
    "    attn_out = MultiHeadAttention(NUM_HEADS, key_dim=D_MODEL)(\n",
    "        x, x, attention_mask=mask\n",
    "    )\n",
    "    attn_out = Dropout(DROPOUT_RATE)(attn_out)\n",
    "    out1     = LayerNormalization(epsilon=1e-6)(x + attn_out)\n",
    "    ffn      = Dense(DFF, activation=\"relu\")(out1)\n",
    "    ffn      = Dense(D_MODEL)(ffn)\n",
    "    ffn      = Dropout(DROPOUT_RATE)(ffn)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn)\n",
    "\n",
    "def transformer_decoder_layer(x, enc_out, *, look_ahead_mask=None, padding_mask=None):\n",
    "    attn1 = MultiHeadAttention(NUM_HEADS, key_dim=D_MODEL)(\n",
    "        x, x, attention_mask=look_ahead_mask\n",
    "    )\n",
    "    attn1 = Dropout(DROPOUT_RATE)(attn1)\n",
    "    out1  = LayerNormalization(epsilon=1e-6)(x + attn1)\n",
    "\n",
    "    attn2 = MultiHeadAttention(NUM_HEADS, key_dim=D_MODEL)(\n",
    "        out1, enc_out, enc_out, attention_mask=padding_mask\n",
    "    )\n",
    "    attn2 = Dropout(DROPOUT_RATE)(attn2)\n",
    "    out2  = LayerNormalization(epsilon=1e-6)(out1 + attn2)\n",
    "\n",
    "    ffn   = Dense(DFF, activation=\"relu\")(out2)\n",
    "    ffn   = Dense(D_MODEL)(ffn)\n",
    "    ffn   = Dropout(DROPOUT_RATE)(ffn)\n",
    "    return LayerNormalization(epsilon=1e-6)(out2 + ffn)\n",
    "\n",
    "# ─── INPUTS ───────────────────────────────────────────────────────\n",
    "ctx_input = Input(shape=(MAX_CTX_LEN,), name=\"context_input\")\n",
    "q_input   = Input(shape=(MAX_Q_LEN,),   name=\"question_input\")\n",
    "ans_input = Input(shape=(MAX_ANS_LEN,), name=\"answer_input\")\n",
    "\n",
    "# ─── SOURCE CONCAT & EMBEDDINGS ───────────────────────────────────\n",
    "src     = Concatenate(name=\"src_concat\")([ctx_input, q_input])\n",
    "embed   = Embedding(VOCAB_SIZE, D_MODEL, name=\"token_embedding\")\n",
    "src_emb = embed(src)\n",
    "tgt_emb = embed(ans_input)\n",
    "\n",
    "# ─── ADD POSITIONAL ENCODING ──────────────────────────────────────\n",
    "src_emb = PositionalEncoding(MAX_SRC_LEN, D_MODEL, name=\"pos_enc_src\")(src_emb)\n",
    "tgt_emb = PositionalEncoding(MAX_ANS_LEN, D_MODEL, name=\"pos_enc_tgt\")(tgt_emb)\n",
    "\n",
    "# ─── MASKS VIA LAMBDA ─────────────────────────────────────────────\n",
    "enc_padding_mask = Lambda(\n",
    "    lambda seq: tf.cast(tf.equal(seq, 0), tf.float32)[:, None, None, :],\n",
    "    name=\"enc_padding_mask\"\n",
    ")(src)\n",
    "\n",
    "look_ahead_mask = Lambda(\n",
    "    lambda x: 1 - tf.linalg.band_part(\n",
    "        tf.ones((MAX_ANS_LEN, MAX_ANS_LEN)), -1, 0\n",
    "    )[None, None, :, :],\n",
    "    name=\"look_ahead_mask\"\n",
    ")(ans_input)\n",
    "\n",
    "dec_padding_mask = enc_padding_mask\n",
    "\n",
    "# ─── ENCODER STACK ────────────────────────────────────────────────\n",
    "enc_out = src_emb\n",
    "for i in range(NUM_LAYERS):\n",
    "    enc_out = transformer_encoder_layer(\n",
    "        enc_out, mask=enc_padding_mask\n",
    "    )\n",
    "\n",
    "# ─── DECODER STACK ────────────────────────────────────────────────\n",
    "dec_out = tgt_emb\n",
    "for i in range(NUM_LAYERS):\n",
    "    dec_out = transformer_decoder_layer(\n",
    "        dec_out,\n",
    "        enc_out,\n",
    "        look_ahead_mask=look_ahead_mask,\n",
    "        padding_mask=dec_padding_mask\n",
    "    )\n",
    "\n",
    "# ─── FINAL PROJECTION ─────────────────────────────────────────────\n",
    "logits = Dense(VOCAB_SIZE, name=\"decoder_output\")(dec_out)\n",
    "logits = TemperatureScaling(temperature=0.7)(logits)\n",
    "logits = tf.keras.layers.Activation(\"softmax\")(logits)\n",
    "\n",
    "\n",
    "# ─── MODEL & COMPILATION ────────────────────────────────────────\n",
    "model = Model(\n",
    "    inputs=[ctx_input, q_input, ans_input],\n",
    "    outputs=logits,\n",
    "    name=\"Transformer_QA_Seq2Seq\"\n",
    ")\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.98,\n",
    "    epsilon=1e-9,\n",
    "    clipnorm=CLIP_NORM,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=masked_loss,\n",
    "    metrics=[masked_accuracy],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T16:51:50.662986Z",
     "iopub.status.busy": "2025-04-22T16:51:50.662749Z",
     "iopub.status.idle": "2025-04-22T17:25:41.472548Z",
     "shell.execute_reply": "2025-04-22T17:25:41.471778Z",
     "shell.execute_reply.started": "2025-04-22T16:51:50.662969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745340743.966918     113 service.cc:148] XLA service 0x7ec4000234c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745340743.967769     113 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1745340743.967788     113 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1745340748.319395     113 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1745340768.772024     113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 541ms/step - loss: 9.1142 - masked_accuracy: 0.2171 - val_loss: 7.7173 - val_masked_accuracy: 0.2412\n",
      "Epoch 2/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 422ms/step - loss: 7.2036 - masked_accuracy: 0.2376 - val_loss: 6.9142 - val_masked_accuracy: 0.2619\n",
      "Epoch 3/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 410ms/step - loss: 6.4614 - masked_accuracy: 0.2765 - val_loss: 6.5347 - val_masked_accuracy: 0.3045\n",
      "Epoch 4/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 417ms/step - loss: 6.0466 - masked_accuracy: 0.3176 - val_loss: 6.2968 - val_masked_accuracy: 0.3266\n",
      "Epoch 5/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 413ms/step - loss: 5.7872 - masked_accuracy: 0.3429 - val_loss: 6.1289 - val_masked_accuracy: 0.3593\n",
      "Epoch 6/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 412ms/step - loss: 5.5724 - masked_accuracy: 0.3700 - val_loss: 5.9936 - val_masked_accuracy: 0.3781\n",
      "Epoch 7/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 413ms/step - loss: 5.3749 - masked_accuracy: 0.3877 - val_loss: 5.8802 - val_masked_accuracy: 0.3895\n",
      "Epoch 8/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 5.2220 - masked_accuracy: 0.3968 - val_loss: 5.7835 - val_masked_accuracy: 0.3946\n",
      "Epoch 9/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 5.1118 - masked_accuracy: 0.3997 - val_loss: 5.6978 - val_masked_accuracy: 0.4022\n",
      "Epoch 10/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 413ms/step - loss: 4.9613 - masked_accuracy: 0.4157 - val_loss: 5.6031 - val_masked_accuracy: 0.4142\n",
      "Epoch 11/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.8806 - masked_accuracy: 0.4219 - val_loss: 5.5240 - val_masked_accuracy: 0.4207\n",
      "Epoch 12/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 413ms/step - loss: 4.7623 - masked_accuracy: 0.4314 - val_loss: 5.4734 - val_masked_accuracy: 0.4198\n",
      "Epoch 13/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 413ms/step - loss: 4.6766 - masked_accuracy: 0.4399 - val_loss: 5.4031 - val_masked_accuracy: 0.4281\n",
      "Epoch 14/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.5984 - masked_accuracy: 0.4448 - val_loss: 5.3541 - val_masked_accuracy: 0.4474\n",
      "Epoch 15/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.5095 - masked_accuracy: 0.4599 - val_loss: 5.3010 - val_masked_accuracy: 0.4547\n",
      "Epoch 16/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.4312 - masked_accuracy: 0.4672 - val_loss: 5.2366 - val_masked_accuracy: 0.4634\n",
      "Epoch 17/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.3511 - masked_accuracy: 0.4818 - val_loss: 5.2082 - val_masked_accuracy: 0.4580\n",
      "Epoch 18/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.2867 - masked_accuracy: 0.4886 - val_loss: 5.1412 - val_masked_accuracy: 0.4827\n",
      "Epoch 19/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 413ms/step - loss: 4.1966 - masked_accuracy: 0.5071 - val_loss: 5.1100 - val_masked_accuracy: 0.4848\n",
      "Epoch 20/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.1611 - masked_accuracy: 0.5092 - val_loss: 5.0621 - val_masked_accuracy: 0.4896\n",
      "Epoch 21/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.0875 - masked_accuracy: 0.5204 - val_loss: 5.0071 - val_masked_accuracy: 0.4916\n",
      "Epoch 22/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 414ms/step - loss: 4.0108 - masked_accuracy: 0.5326 - val_loss: 4.9432 - val_masked_accuracy: 0.5056\n",
      "Epoch 23/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 415ms/step - loss: 3.9366 - masked_accuracy: 0.5434 - val_loss: 4.8983 - val_masked_accuracy: 0.5250\n",
      "Epoch 24/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 418ms/step - loss: 3.8815 - masked_accuracy: 0.5556 - val_loss: 4.8817 - val_masked_accuracy: 0.5148\n",
      "Epoch 25/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 418ms/step - loss: 3.7989 - masked_accuracy: 0.5641 - val_loss: 4.8716 - val_masked_accuracy: 0.5252\n",
      "Epoch 26/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 419ms/step - loss: 3.7675 - masked_accuracy: 0.5712 - val_loss: 4.7776 - val_masked_accuracy: 0.5430\n",
      "Epoch 27/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 419ms/step - loss: 3.6868 - masked_accuracy: 0.5882 - val_loss: 4.7684 - val_masked_accuracy: 0.5410\n",
      "Epoch 28/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 417ms/step - loss: 3.6380 - masked_accuracy: 0.5955 - val_loss: 4.7212 - val_masked_accuracy: 0.5545\n",
      "Epoch 29/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 416ms/step - loss: 3.5719 - masked_accuracy: 0.6023 - val_loss: 4.6645 - val_masked_accuracy: 0.5657\n",
      "Epoch 30/30\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 415ms/step - loss: 3.5249 - masked_accuracy: 0.6183 - val_loss: 4.6369 - val_masked_accuracy: 0.5631\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=[X_ctx_train, X_q_train, X_ans_in_tr],\n",
    "    y=y_ans_train,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_data=(\n",
    "        [X_ctx_val, X_q_val, X_ans_in_val],\n",
    "        y_ans_valid\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:34:42.665692Z",
     "iopub.status.busy": "2025-04-22T17:34:42.665360Z",
     "iopub.status.idle": "2025-04-22T17:34:44.741234Z",
     "shell.execute_reply": "2025-04-22T17:34:44.740465Z",
     "shell.execute_reply.started": "2025-04-22T17:34:42.665660Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing generation with new parameters...\n",
      "\n",
      "================================================================================\n",
      "Context:   Private schooling in the United States has been debated by educators, lawmakers and parents, since the beginnings of compulsory education in Massachusetts in 1852. The Supreme Court precedent appears to favor educational choice, so long as states may set standards for educational accomplishment. Some of the most relevant Supreme Court case law on this is as follows: Runyon v. McCrary, 427 U.S. 160 (1976); Wisconsin v. Yoder, 406 U.S. 205 (1972); Pierce v. Society of Sisters, 268 U.S. 510 (1925); Meyer v. Nebraska, 262 U.S. 390 (1923).\n",
      "\n",
      "Question:  In what year did Massachusetts first require children to be educated in schools?\n",
      "Step 1: Selected token 'war' with probability 0.0556\n",
      "Step 2: Selected token '000' with probability 0.0556\n",
      "Step 3: Selected token '<end>' with probability 0.2189\n",
      "\n",
      "Generation summary:\n",
      "Generated tokens: war 000 <end>\n",
      "Probabilities: ['war: 0.2189']\n",
      "\n",
      "Predicted: war 000\n",
      "Ground Truth: 1852\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Context:   The chloroplast membranes sometimes protrude out into the cytoplasm, forming a stromule, or stroma-containing tubule. Stromules are very rare in chloroplasts, and are much more common in other plastids like chromoplasts and amyloplasts in petals and roots, respectively. They may exist to increase the chloroplast's surface area for cross-membrane transport, because they are often branched and tangled with the endoplasmic reticulum. When they were first observed in 1962, some plant biologists dismissed the structures as artifactual, claiming that stromules were just oddly shaped chloroplasts with constricted regions or dividing chloroplasts. However, there is a growing body of evidence that stromules are functional, integral features of plant cell plastids, not merely artifacts.\n",
      "\n",
      "Question:  When were stromules discovered?\n",
      "Step 1: Selected token 'billion' with probability 0.0556\n",
      "Step 2: Selected token 'and' with probability 0.0556\n",
      "Step 3: Selected token '22' with probability 0.0460\n",
      "Step 4: Selected token '<end>' with probability 0.2188\n",
      "\n",
      "Generation summary:\n",
      "Generated tokens: billion and 22 <end>\n",
      "Probabilities: ['billion: 0.2188']\n",
      "\n",
      "Predicted: billion and 22\n",
      "Ground Truth: 1962\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Context:   Not only the work of British artists and craftspeople is on display, but also work produced by European artists that was purchased or commissioned by British patrons, as well as imports from Asia, including porcelain, cloth and wallpaper. Designers and artists whose work is on display in the galleries include Gian Lorenzo Bernini, Grinling Gibbons, Daniel Marot, Louis Laguerre, Antonio Verrio, Sir James Thornhill, William Kent, Robert Adam, Josiah Wedgwood, Matthew Boulton, Canova, Thomas Chippendale, Pugin, William Morris. Patrons who have influenced taste are also represented by works of art from their collections, these include: Horace Walpole (a major influence on the Gothic Revival), William Thomas Beckford and Thomas Hope.\n",
      "\n",
      "Question:  Which artist who had a major influence on the Gothic Revival is represented in the V&A's British galleries?\n",
      "Step 1: Selected token 'soviet' with probability 0.0556\n",
      "Step 2: Selected token '<UNK>' with probability 0.0556\n",
      "Step 3: Selected token '000' with probability 0.0460\n",
      "Step 4: Selected token 'per' with probability 0.0459\n",
      "Step 5: Selected token 'united' with probability 0.0459\n",
      "Step 6: Selected token 'and' with probability 0.0459\n",
      "Step 7: Selected token '<end>' with probability 0.2189\n",
      "\n",
      "Generation summary:\n",
      "Generated tokens: soviet <UNK> 000 per united and <end>\n",
      "Probabilities: ['soviet: 0.2189']\n",
      "\n",
      "Predicted: soviet 000 per united and\n",
      "Ground Truth: Horace Walpole\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def generate_answer_transformer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    context,\n",
    "    question,\n",
    "    max_ctx_len=MAX_CTX_LEN,\n",
    "    max_q_len=MAX_Q_LEN,\n",
    "    max_ans_len=MAX_ANS_LEN,\n",
    "    temperature=0.6,\n",
    "    top_k=20,\n",
    "    top_p=0.9,\n",
    "    min_length=3,\n",
    "):\n",
    "    # 1) Encode & pad context + question\n",
    "    ctx_seq = tokenizer.texts_to_sequences([context])\n",
    "    ctx_seq = pad_sequences(ctx_seq, maxlen=max_ctx_len, padding=\"post\")\n",
    "\n",
    "    q_seq = tokenizer.texts_to_sequences([question])\n",
    "    q_seq = pad_sequences(q_seq, maxlen=max_q_len, padding=\"post\")\n",
    "\n",
    "    # 2) Prepare decoder input seeded with <start>\n",
    "    start_idx = tokenizer.word_index[\"<start>\"]\n",
    "    end_idx = tokenizer.word_index[\"<end>\"]\n",
    "    pad_idx = 0\n",
    "    ans_seq = np.zeros((1, max_ans_len), dtype=\"int32\")\n",
    "    ans_seq[0, 0] = start_idx\n",
    "\n",
    "    # Get reverse word index for debugging\n",
    "    idx2word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    generated_tokens = []\n",
    "\n",
    "    def nucleus_sampling(logits, temperature, top_k, top_p):\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Remove low probability tokens\n",
    "        if top_k > 0:\n",
    "            top_k_logits, top_k_indices = tf.math.top_k(\n",
    "                logits, k=min(top_k, logits.shape[-1])\n",
    "            )\n",
    "        else:\n",
    "            top_k_logits, top_k_indices = logits, tf.range(logits.shape[-1])\n",
    "\n",
    "        # Apply softmax to convert to probabilities\n",
    "        probs = tf.nn.softmax(top_k_logits)\n",
    "\n",
    "        # Nucleus (top-p) sampling\n",
    "        if top_p < 1.0:\n",
    "            sorted_probs = tf.sort(probs, direction=\"DESCENDING\")\n",
    "            cumsum_probs = tf.cumsum(sorted_probs)\n",
    "            cutoff = tf.where(cumsum_probs > top_p)[0][0]\n",
    "            probs = tf.where(probs < sorted_probs[cutoff], tf.zeros_like(probs), probs)\n",
    "            # Renormalize probabilities\n",
    "            probs = probs / tf.reduce_sum(probs)\n",
    "\n",
    "        # Sample from the filtered distribution\n",
    "        sample_idx = tf.random.categorical(tf.math.log(probs[None, :]), num_samples=1)[\n",
    "            0, 0\n",
    "        ]\n",
    "        return top_k_indices[sample_idx], probs[sample_idx]\n",
    "\n",
    "    # 3) Generate with nucleus sampling\n",
    "    for i in range(1, max_ans_len):\n",
    "        # Get model predictions\n",
    "        preds = model.predict([ctx_seq, q_seq, ans_seq], verbose=0)\n",
    "        next_token_logits = preds[0, i - 1]\n",
    "\n",
    "        # Mask padding token\n",
    "        next_token_logits = tf.where(\n",
    "            tf.range(len(next_token_logits)) == pad_idx,\n",
    "            tf.fill(next_token_logits.shape, -1e9),\n",
    "            next_token_logits,\n",
    "        )\n",
    "\n",
    "        # Don't allow END token before min_length\n",
    "        if i < min_length:\n",
    "            next_token_logits = tf.where(\n",
    "                tf.range(len(next_token_logits)) == end_idx,\n",
    "                tf.fill(next_token_logits.shape, -1e9),\n",
    "                next_token_logits,\n",
    "            )\n",
    "\n",
    "        # Apply nucleus sampling\n",
    "        next_id, prob = nucleus_sampling(next_token_logits, temperature, top_k, top_p)\n",
    "\n",
    "        # Print debugging info\n",
    "        token = idx2word.get(next_id.numpy(), \"<UNK>\")\n",
    "        print(f\"Step {i}: Selected token '{token}' with probability {prob:.4f}\")\n",
    "\n",
    "        # Save the generated token\n",
    "        generated_tokens.append(token)\n",
    "\n",
    "        # Update sequence\n",
    "        ans_seq[0, i] = next_id\n",
    "\n",
    "        # Break if we generate END token\n",
    "        if next_id == end_idx:\n",
    "            break\n",
    "\n",
    "    # Print full generation info\n",
    "    print(\"\\nGeneration summary:\")\n",
    "    print(\"Generated tokens:\", \" \".join(generated_tokens))\n",
    "    print(\"Probabilities:\", [f\"{t}: {p:.4f}\" for t, p in zip(generated_tokens, [prob])])\n",
    "\n",
    "    # 4) Convert IDs back to words, skipping special tokens\n",
    "    output = []\n",
    "    for idx in ans_seq[0]:\n",
    "        if idx == pad_idx or idx == start_idx:\n",
    "            continue\n",
    "        if idx == end_idx:\n",
    "            break\n",
    "        token = idx2word.get(idx, \"\")\n",
    "        if token and not token.startswith(\"<\"):\n",
    "            output.append(token)\n",
    "\n",
    "    return \" \".join(output)\n",
    "\n",
    "\n",
    "# ─── Test on one example ─────────────────────────────────────────────\n",
    "print(\"\\nTesting generation with new parameters...\")\n",
    "for index in range(3):  # Test on fewer examples\n",
    "    example = squad_dataset[\"validation\"][index]\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    ground_truth = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Context:  \", context)\n",
    "    print(\"\\nQuestion: \", question)\n",
    "    print(\n",
    "        \"\\nPredicted:\",\n",
    "        generate_answer_transformer(\n",
    "            model, tokenizer, context, question, temperature=0.6, top_k=20, top_p=0.9\n",
    "        ),\n",
    "    )\n",
    "    print(\"Ground Truth:\", ground_truth)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T17:57:12.838302Z",
     "iopub.status.busy": "2025-04-22T17:57:12.837599Z",
     "iopub.status.idle": "2025-04-22T17:57:14.159390Z",
     "shell.execute_reply": "2025-04-22T17:57:14.158771Z",
     "shell.execute_reply.started": "2025-04-22T17:57:12.838279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/54687702.py:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history plots have been saved to 'training_history.png'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Create figure with two subplots\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Plot Loss\n",
    "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
    "ax1.plot(epochs, history.history[\"loss\"], \"b-\", label=\"Training Loss\")\n",
    "ax1.plot(epochs, history.history[\"val_loss\"], \"r-\", label=\"Validation Loss\")\n",
    "ax1.set_title(\"Model Loss Over Time\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Add final values annotation\n",
    "ax1.annotate(\n",
    "    f\"Final Train Loss: {history.history['loss'][-1]:.4f}\",\n",
    "    xy=(epochs[-1], history.history[\"loss\"][-1]),\n",
    "    xytext=(epochs[-1] - 2, history.history[\"loss\"][-1] + 0.2),\n",
    "    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n",
    ")\n",
    "ax1.annotate(\n",
    "    f\"Final Val Loss: {history.history['val_loss'][-1]:.4f}\",\n",
    "    xy=(epochs[-1], history.history[\"val_loss\"][-1]),\n",
    "    xytext=(epochs[-1] - 2, history.history[\"val_loss\"][-1] + 0.2),\n",
    "    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n",
    ")\n",
    "\n",
    "# Plot Accuracy\n",
    "ax2.plot(epochs, history.history[\"masked_accuracy\"], \"b-\", label=\"Training Accuracy\")\n",
    "ax2.plot(epochs, history.history[\"val_masked_accuracy\"], \"r-\", label=\"Validation Accuracy\")\n",
    "ax2.set_title(\"Model Accuracy Over Time\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Masked Accuracy\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Add final values annotation\n",
    "ax2.annotate(\n",
    "    f\"Final Train Acc: {history.history['masked_accuracy'][-1]:.4f}\",\n",
    "    xy=(epochs[-1], history.history[\"masked_accuracy\"][-1]),\n",
    "    xytext=(epochs[-1] - 2, history.history[\"masked_accuracy\"][-1] - 0.05),\n",
    "    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n",
    ")\n",
    "ax2.annotate(\n",
    "    f\"Final Val Acc: {history.history['val_masked_accuracy'][-1]:.4f}\",\n",
    "    xy=(epochs[-1], history.history[\"val_masked_accuracy\"][-1]),\n",
    "    xytext=(epochs[-1] - 2, history.history[\"val_masked_accuracy\"][-1] - 0.05),\n",
    "    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n",
    ")\n",
    "\n",
    "# Add analysis text\n",
    "plt.figtext(\n",
    "    0.02,\n",
    "    0.02,\n",
    "    \"\"\"\n",
    "Analysis:\n",
    "- Training converges but shows signs of overfitting (validation loss increases)\n",
    "- Final validation loss (4.64) is ~31% higher than training loss (3.52)\n",
    "- Accuracy improvement slows after epoch 20\n",
    "- Possible solutions: early stopping, stronger regularization, or learning rate decay\n",
    "\"\"\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(facecolor=\"white\", alpha=0.8),\n",
    ")\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_history.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Training history plots have been saved to 'training_history.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
