{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-17T23:11:02.540191Z",
     "iopub.status.busy": "2025-05-17T23:11:02.539911Z",
     "iopub.status.idle": "2025-05-17T23:12:28.780694Z",
     "shell.execute_reply": "2025-05-17T23:12:28.779721Z",
     "shell.execute_reply.started": "2025-05-17T23:11:02.540159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.3.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Collecting langchain-core<1.0.0,>=0.3.49 (from langchain)\n",
      "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.31.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Collecting ollama<1.0.0,>=0.4.8 (from langchain-ollama)\n",
      "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (1.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
      "Collecting packaging (from faiss-cpu)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain_community) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_ollama-0.3.3-py3-none-any.whl (21 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e73cfc988d0c9f06c36ada784d338cd4fe7fb79aa92d8935c6e1cdb8e6ffc126\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: python-dotenv, packaging, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, httpx-sse, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, ollama, nvidia-cusolver-cu12, langchain-core, langchain-text-splitters, langchain-ollama, langchain, rouge-score, langchain_huggingface, langchain_community, faiss-cpu\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.50\n",
      "    Uninstalling langchain-core-0.3.50:\n",
      "      Successfully uninstalled langchain-core-0.3.50\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.7\n",
      "    Uninstalling langchain-text-splitters-0.3.7:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.7\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.22\n",
      "    Uninstalling langchain-0.3.22:\n",
      "      Successfully uninstalled langchain-0.3.22\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed faiss-cpu-1.11.0 fsspec-2025.3.0 httpx-sse-0.4.0 langchain-0.3.25 langchain-core-0.3.60 langchain-ollama-0.3.3 langchain-text-splitters-0.3.8 langchain_community-0.3.24 langchain_huggingface-0.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ollama-0.4.8 packaging-24.2 pydantic-settings-2.9.1 python-dotenv-1.1.0 rouge-score-0.1.2\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community langchain_huggingface faiss-cpu datasets sentence-transformers transformers langchain-ollama rouge-score\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:12:28.785046Z",
     "iopub.status.busy": "2025-05-17T23:12:28.784546Z",
     "iopub.status.idle": "2025-05-17T23:13:09.568802Z",
     "shell.execute_reply": "2025-05-17T23:13:09.567838Z",
     "shell.execute_reply.started": "2025-05-17T23:12:28.785010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "############################################################################################# 100.0%########                                                                    29.8% 47.1%####################################################                                68.2%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# Install ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:19:31.672731Z",
     "iopub.status.busy": "2025-05-17T23:19:31.672175Z",
     "iopub.status.idle": "2025-05-17T23:19:33.055601Z",
     "shell.execute_reply": "2025-05-17T23:19:33.054720Z",
     "shell.execute_reply.started": "2025-05-17T23:19:31.672707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 6a0746a1ec1a: 100% ▕██████████████████▏ 4.7 GB                         \u001b[K\n",
      "pulling 4fa551d4f938: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 8ab4849b038c: 100% ▕██████████████████▏  254 B                         \u001b[K\n",
      "pulling 577073ffcc6c: 100% ▕██████████████████▏  110 B                         \u001b[K\n",
      "pulling 3f8eb4da87fa: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start the server\n",
    "server_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Give it a moment to start\n",
    "time.sleep(1)\n",
    "\n",
    "# Now run the client command\n",
    "subprocess.run(['ollama', 'pull', 'llama3'])\n",
    "\n",
    "\n",
    "# Kill the server when done\n",
    "server_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:19:36.057195Z",
     "iopub.status.busy": "2025-05-17T23:19:36.056892Z",
     "iopub.status.idle": "2025-05-17T23:19:40.923786Z",
     "shell.execute_reply": "2025-05-17T23:19:40.923242Z",
     "shell.execute_reply.started": "2025-05-17T23:19:36.057173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:19:46.061227Z",
     "iopub.status.busy": "2025-05-17T23:19:46.060412Z",
     "iopub.status.idle": "2025-05-17T23:19:48.578136Z",
     "shell.execute_reply": "2025-05-17T23:19:48.577352Z",
     "shell.execute_reply.started": "2025-05-17T23:19:46.061194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5810ddb3f04c3a914ee02dbad138a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99cb0ee3f86412cb41153e937ce9963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbff9ab14d844c4ab166c83b1c5862d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ff59ec1d5147b2acc890dce261b4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e790d27f0d04993a3d650346fa3c7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 11873\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the entire SQuAD v2 validation set\n",
    "dataset = load_dataset(\"squad_v2\", split=\"validation\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:19:50.718205Z",
     "iopub.status.busy": "2025-05-17T23:19:50.717889Z",
     "iopub.status.idle": "2025-05-17T23:19:51.502285Z",
     "shell.execute_reply": "2025-05-17T23:19:51.501453Z",
     "shell.execute_reply.started": "2025-05-17T23:19:50.718183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Prepare documents (context passages)\n",
    "documents = [item[\"context\"] for item in dataset]\n",
    "\n",
    "# Remove duplicates\n",
    "documents = list(set(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:19:51.863857Z",
     "iopub.status.busy": "2025-05-17T23:19:51.862991Z",
     "iopub.status.idle": "2025-05-17T23:19:51.868655Z",
     "shell.execute_reply": "2025-05-17T23:19:51.867922Z",
     "shell.execute_reply.started": "2025-05-17T23:19:51.863826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:19:56.033540Z",
     "iopub.status.busy": "2025-05-17T23:19:56.032846Z",
     "iopub.status.idle": "2025-05-17T23:19:56.038931Z",
     "shell.execute_reply": "2025-05-17T23:19:56.038289Z",
     "shell.execute_reply.started": "2025-05-17T23:19:56.033514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56ddde6b9a695914005b9628',\n",
       " 'title': 'Normans',\n",
       " 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n",
       " 'question': 'In what country is Normandy located?',\n",
       " 'answers': {'text': ['France', 'France', 'France', 'France'],\n",
       "  'answer_start': [159, 159, 159, 159]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:20:04.028725Z",
     "iopub.status.busy": "2025-05-17T23:20:04.027997Z",
     "iopub.status.idle": "2025-05-17T23:20:36.956077Z",
     "shell.execute_reply": "2025-05-17T23:20:36.955485Z",
     "shell.execute_reply.started": "2025-05-17T23:20:04.028699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3854753891.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "2025-05-17 23:20:15.492494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747524015.677342      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747524015.731199      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38061078f3b74f5ba53cdfb6fea6a7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f0a0332e5b4955a2b7d80a238889c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/68.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b18ea415b44a978c2b52475a3213fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8618bf3788c44586bd5cad471c7b2c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bbccf4fdfe497993e9ef2f4e48cf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8375d626dd747c9959e7dcfdd2f8a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/362 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813859b4c9df4cde8edd9b19cb9cf8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb5ebffcfd14e9ea164ca3faac14d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3161777ed6b149e5b0654707ad6c4bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed05851329f4423a54372c04202a12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Use e5 small embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-small\", model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:20:58.665347Z",
     "iopub.status.busy": "2025-05-17T23:20:58.664626Z",
     "iopub.status.idle": "2025-05-17T23:21:03.646902Z",
     "shell.execute_reply": "2025-05-17T23:21:03.646319Z",
     "shell.execute_reply.started": "2025-05-17T23:20:58.665321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. Create FAISS in-memory vector store (This will take a couple of minutes)\n",
    "vectorstore = FAISS.from_texts(documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:21:18.820381Z",
     "iopub.status.busy": "2025-05-17T23:21:18.820042Z",
     "iopub.status.idle": "2025-05-17T23:21:18.824274Z",
     "shell.execute_reply": "2025-05-17T23:21:18.823430Z",
     "shell.execute_reply.started": "2025-05-17T23:21:18.820359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieve top 5 documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:21:29.897818Z",
     "iopub.status.busy": "2025-05-17T23:21:29.897004Z",
     "iopub.status.idle": "2025-05-17T23:21:30.014882Z",
     "shell.execute_reply": "2025-05-17T23:21:30.014021Z",
     "shell.execute_reply.started": "2025-05-17T23:21:29.897782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#5. Initialize the LLM with Llama 3.3 8b model\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:21:40.592976Z",
     "iopub.status.busy": "2025-05-17T23:21:40.592463Z",
     "iopub.status.idle": "2025-05-17T23:21:40.597888Z",
     "shell.execute_reply": "2025-05-17T23:21:40.597221Z",
     "shell.execute_reply.started": "2025-05-17T23:21:40.592953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the question based only on the following context:\\n{context}, Just type Answer: <YOUR ANSWER>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "# 7. RAG chain\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:21:43.329014Z",
     "iopub.status.busy": "2025-05-17T23:21:43.328702Z",
     "iopub.status.idle": "2025-05-17T23:21:48.603061Z",
     "shell.execute_reply": "2025-05-17T23:21:48.602155Z",
     "shell.execute_reply.started": "2025-05-17T23:21:43.328992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In what country is Normandy located?\n",
      "Answer: France\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Document 1:\n",
      "The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
      "\n",
      "Document 2:\n",
      "In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal property. The Duchy of Normandy, which began in 911 as a fiefdom, was established by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdom of Neustria. The treaty offered Rollo and his men the French lands between the river Epte and the Atlantic coast in exchange for their protection against further Viking incursions. The area corresponded to the northern part of present-day Upper Normandy down to the river Seine, but the Duchy would eventually extend west beyond the Seine. The territory was roughly equivalent to the old province of Rouen, and reproduced the Roman administrative structure of Gallia Lugdunensis II (part of the former Gallia Lugdunensis).\n",
      "\n",
      "Document 3:\n",
      "In 1066, Duke William II of Normandy conquered England killing King Harold II at the Battle of Hastings. The invading Normans and their descendants replaced the Anglo-Saxons as the ruling class of England. The nobility of England were part of a single Normans culture and many had lands on both sides of the channel. Early Norman kings of England, as Dukes of Normandy, owed homage to the King of France for their land on the continent. They considered England to be their most important holding (it brought with it the title of King—an important status symbol).\n",
      "\n",
      "Document 4:\n",
      "Normandy was the site of several important developments in the history of classical music in the 11th century. Fécamp Abbey and Saint-Evroul Abbey were centres of musical production and education. At Fécamp, under two Italian abbots, William of Volpiano and John of Ravenna, the system of denoting notes by letters was developed and taught. It is still the most common form of pitch representation in English- and German-speaking countries today. Also at Fécamp, the staff, around which neumes were oriented, was first developed and taught in the 11th century. Under the German abbot Isembard, La Trinité-du-Mont became a centre of musical composition.\n",
      "\n",
      "Document 5:\n",
      "The customary law of Normandy was developed between the 10th and 13th centuries and survives today through the legal systems of Jersey and Guernsey in the Channel Islands. Norman customary law was transcribed in two customaries in Latin by two judges for use by them and their colleagues: These are the Très ancien coutumier (Very ancient customary), authored between 1200 and 1245; and the Grand coutumier de Normandie (Great customary of Normandy, originally Summa de legibus Normanniae in curia laïcali), authored between 1235 and 1245.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3153424737.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "# 8. Example QA\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start the server\n",
    "server_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "time.sleep(0.5)\n",
    "\n",
    "question = dataset[0][\"question\"]\n",
    "print(\"Question:\", question)\n",
    "\n",
    "print(chain.invoke(question))\n",
    "\n",
    "# Retrieve and print the top documents for the question\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "print(\"\\n--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "\n",
    "# Kill the server when done\n",
    "server_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:21:51.105960Z",
     "iopub.status.busy": "2025-05-17T23:21:51.105408Z",
     "iopub.status.idle": "2025-05-17T23:21:51.114478Z",
     "shell.execute_reply": "2025-05-17T23:21:51.113585Z",
     "shell.execute_reply.started": "2025-05-17T23:21:51.105940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8. Evaluation on the entire validation dataset\n",
    "def evaluate_rag_system(dataset, chain, sample_size=None, is_cot=False):\n",
    "    \"\"\"\n",
    "    Evaluate the RAG system on the SQuAD v2 validation dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: The SQuAD v2 validation dataset\n",
    "        chain: The RAG chain to evaluate\n",
    "        sample_size: Optional number of samples to evaluate (for testing)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    server_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    results = []\n",
    "    # Track metrics totals\n",
    "    total_rouge1 = 0.0\n",
    "    total_rouge2 = 0.0\n",
    "    total_rougeL = 0.0\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Limit the number of samples if specified\n",
    "    if sample_size:\n",
    "        eval_dataset = dataset.select(range(sample_size))\n",
    "    else:\n",
    "        eval_dataset = dataset\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
    "        question = item[\"question\"]\n",
    "        ground_truth_answers = item[\"answers\"][\"text\"]\n",
    "        if len(ground_truth_answers) == 0:\n",
    "            continue\n",
    "        has_answer = len(ground_truth_answers) > 0\n",
    "\n",
    "        try:\n",
    "            # Get prediction from the RAG system\n",
    "            prediction = chain.invoke(question)\n",
    "            # Calculate ROUGE scores\n",
    "            rouge_scores = calculate_rouge_scores(prediction, ground_truth_answers, is_cot)\n",
    "\n",
    "            # Update totals\n",
    "            total_rouge1 += rouge_scores[\"rouge1\"]\n",
    "            total_rouge2 += rouge_scores[\"rouge2\"]\n",
    "            total_rougeL += rouge_scores[\"rougeL\"]\n",
    "            processed_count += 1\n",
    "\n",
    "            # Store result\n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"prediction\": prediction,\n",
    "                \"ground_truth_answers\": ground_truth_answers,\n",
    "                \"has_answer\": has_answer,\n",
    "                \"context\": item[\"context\"],\n",
    "                \"rouge_scores\": rouge_scores\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {question}\")\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time_per_question = (\n",
    "        total_time / len(eval_dataset) if len(eval_dataset) > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_rouge1 = total_rouge1 / processed_count if processed_count > 0 else 0\n",
    "    avg_rouge2 = total_rouge2 / processed_count if processed_count > 0 else 0\n",
    "    avg_rougeL = total_rougeL / processed_count if processed_count > 0 else 0\n",
    "    \n",
    "    # Kill the server when done\n",
    "    server_process.terminate()\n",
    "\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"total_samples\": len(eval_dataset),\n",
    "        \"processed_samples\": processed_count,\n",
    "        \"total_time_seconds\": total_time,\n",
    "        \"avg_time_per_question\": avg_time_per_question,\n",
    "        \"avg_rouge1\": avg_rouge1,\n",
    "        \"avg_rouge2\": avg_rouge2,\n",
    "        \"avg_rougeL\": avg_rougeL\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:22:01.928348Z",
     "iopub.status.busy": "2025-05-17T23:22:01.927650Z",
     "iopub.status.idle": "2025-05-17T23:22:01.933658Z",
     "shell.execute_reply": "2025-05-17T23:22:01.932534Z",
     "shell.execute_reply.started": "2025-05-17T23:22:01.928326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(prediction, ground_truth_answers, is_cot):\n",
    "    \"\"\"Calculate ROUGE scores against all ground truth answers and take the best score.\"\"\"\n",
    "    if not ground_truth_answers:  # No answer case\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "\n",
    "    # Calculate scores against all ground truth answers\n",
    "    best_scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "    if is_cot:\n",
    "        prediction = \"Answer: \" + prediction.rsplit(\"Answer: \", 1)[-1]\n",
    "        \n",
    "    for answer in ground_truth_answers:\n",
    "        scores = scorer.score(prediction, answer)\n",
    "\n",
    "        # Update best scores\n",
    "        for metric in best_scores:\n",
    "            best_scores[metric] = max(best_scores[metric], scores[metric].fmeasure)\n",
    "\n",
    "    return best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:22:12.793105Z",
     "iopub.status.busy": "2025-05-17T23:22:12.792425Z",
     "iopub.status.idle": "2025-05-17T23:30:36.646892Z",
     "shell.execute_reply": "2025-05-17T23:30:36.646035Z",
     "shell.execute_reply.started": "2025-05-17T23:22:12.793083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed on 498 samples\n",
      "Total time: 503.33 seconds\n",
      "Average time per question: 0.50 seconds\n",
      "Average ROUGE-1 score: 0.5330\n",
      "Average ROUGE-2 score: 0.3284\n",
      "Average ROUGE-L score: 0.5307\n",
      "Evaluation results saved to evaluation_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with a small sample size first (can be adjusted)\n",
    "print(\"\\n--- Starting Evaluation ---\")\n",
    "sample_size = 1000\n",
    "evaluation_results = evaluate_rag_system(dataset, chain, sample_size)\n",
    "\n",
    "print(f\"\\nEvaluation completed on {evaluation_results['processed_samples']} samples\")\n",
    "print(f\"Total time: {evaluation_results['total_time_seconds']:.2f} seconds\")\n",
    "print(\n",
    "    f\"Average time per question: {evaluation_results['avg_time_per_question']:.2f} seconds\"\n",
    ")\n",
    "\n",
    "print(f\"Average ROUGE-1 score: {evaluation_results['avg_rouge1']:.4f}\")\n",
    "print(f\"Average ROUGE-2 score: {evaluation_results['avg_rouge2']:.4f}\")\n",
    "print(f\"Average ROUGE-L score: {evaluation_results['avg_rougeL']:.4f}\")\n",
    "\n",
    "# Save results to a file\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation results saved to evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Chain of thoughts (CoT) prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:30:54.364594Z",
     "iopub.status.busy": "2025-05-17T23:30:54.364288Z",
     "iopub.status.idle": "2025-05-17T23:30:54.370648Z",
     "shell.execute_reply": "2025-05-17T23:30:54.369887Z",
     "shell.execute_reply.started": "2025-05-17T23:30:54.364570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Answer the question based only on the following context:\n",
    "                {context}\n",
    "                \n",
    "                To answer correctly, please follow these steps:\n",
    "                1. Read the context carefully\n",
    "                2. Identify the key information in the context that relates to the question\n",
    "                3. Reason step by step to find the answer\n",
    "                4. Provide your final answer clearly prefixed with \"Answer: \"\n",
    "                \n",
    "                You MUST structure your response as follows:\n",
    "                Thinking: [Write out your step-by-step reasoning process here, analyzing the context to find relevant information]\n",
    "                Answer: [Your concise final answer]\n",
    "                \n",
    "                Example:\n",
    "                Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia.\n",
    "                \n",
    "                Question: In what country is Normandy located?\n",
    "                \n",
    "                Thinking: The context directly states that Normandy is \"a region in France.\" This clearly indicates that Normandy is located within the country of France. There's no ambiguity in the text about this fact.\n",
    "                \n",
    "                Answer: France\n",
    "                \"\"\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "# 7. RAG chain\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:30:54.631372Z",
     "iopub.status.busy": "2025-05-17T23:30:54.631053Z",
     "iopub.status.idle": "2025-05-17T23:31:01.809635Z",
     "shell.execute_reply": "2025-05-17T23:31:01.808688Z",
     "shell.execute_reply.started": "2025-05-17T23:30:54.631351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In what country is Normandy located?\n",
      "Thinking: According to the context, Normandy is described as a region in France. The Duchy of Normandy was established by Rollo and King Charles III of West Francia, and it was situated in the former Frankish kingdom of Neustria. This suggests that Normandy is located within the country of France.\n",
      "\n",
      "Answer: France\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Document 1:\n",
      "The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
      "\n",
      "Document 2:\n",
      "In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal property. The Duchy of Normandy, which began in 911 as a fiefdom, was established by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdom of Neustria. The treaty offered Rollo and his men the French lands between the river Epte and the Atlantic coast in exchange for their protection against further Viking incursions. The area corresponded to the northern part of present-day Upper Normandy down to the river Seine, but the Duchy would eventually extend west beyond the Seine. The territory was roughly equivalent to the old province of Rouen, and reproduced the Roman administrative structure of Gallia Lugdunensis II (part of the former Gallia Lugdunensis).\n",
      "\n",
      "Document 3:\n",
      "In 1066, Duke William II of Normandy conquered England killing King Harold II at the Battle of Hastings. The invading Normans and their descendants replaced the Anglo-Saxons as the ruling class of England. The nobility of England were part of a single Normans culture and many had lands on both sides of the channel. Early Norman kings of England, as Dukes of Normandy, owed homage to the King of France for their land on the continent. They considered England to be their most important holding (it brought with it the title of King—an important status symbol).\n",
      "\n",
      "Document 4:\n",
      "Normandy was the site of several important developments in the history of classical music in the 11th century. Fécamp Abbey and Saint-Evroul Abbey were centres of musical production and education. At Fécamp, under two Italian abbots, William of Volpiano and John of Ravenna, the system of denoting notes by letters was developed and taught. It is still the most common form of pitch representation in English- and German-speaking countries today. Also at Fécamp, the staff, around which neumes were oriented, was first developed and taught in the 11th century. Under the German abbot Isembard, La Trinité-du-Mont became a centre of musical composition.\n",
      "\n",
      "Document 5:\n",
      "The customary law of Normandy was developed between the 10th and 13th centuries and survives today through the legal systems of Jersey and Guernsey in the Channel Islands. Norman customary law was transcribed in two customaries in Latin by two judges for use by them and their colleagues: These are the Très ancien coutumier (Very ancient customary), authored between 1200 and 1245; and the Grand coutumier de Normandie (Great customary of Normandy, originally Summa de legibus Normanniae in curia laïcali), authored between 1235 and 1245.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Example QA\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start the server\n",
    "server_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "time.sleep(0.5)\n",
    "\n",
    "question = dataset[0][\"question\"]\n",
    "print(\"Question:\", question)\n",
    "\n",
    "print(chain.invoke(question))\n",
    "\n",
    "# Retrieve and print the top documents for the question\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "print(\"\\n--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "\n",
    "# Kill the server when done\n",
    "server_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T23:31:01.811607Z",
     "iopub.status.busy": "2025-05-17T23:31:01.810862Z",
     "iopub.status.idle": "2025-05-17T23:59:36.712413Z",
     "shell.execute_reply": "2025-05-17T23:59:36.711419Z",
     "shell.execute_reply.started": "2025-05-17T23:31:01.811577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [28:34<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed on 498 samples\n",
      "Total time: 1714.38 seconds\n",
      "Average time per question: 1.71 seconds\n",
      "Average ROUGE-1 score: 0.5202\n",
      "Average ROUGE-2 score: 0.2876\n",
      "Average ROUGE-L score: 0.5193\n",
      "Evaluation results saved to evaluation_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with a small sample size first (can be adjusted)\n",
    "print(\"\\n--- Starting Evaluation ---\")\n",
    "sample_size = 1000\n",
    "evaluation_results = evaluate_rag_system(dataset, chain, sample_size, is_cot=True)\n",
    "\n",
    "print(f\"\\nEvaluation completed on {evaluation_results['processed_samples']} samples\")\n",
    "print(f\"Total time: {evaluation_results['total_time_seconds']:.2f} seconds\")\n",
    "print(\n",
    "    f\"Average time per question: {evaluation_results['avg_time_per_question']:.2f} seconds\"\n",
    ")\n",
    "\n",
    "print(f\"Average ROUGE-1 score: {evaluation_results['avg_rouge1']:.4f}\")\n",
    "print(f\"Average ROUGE-2 score: {evaluation_results['avg_rouge2']:.4f}\")\n",
    "print(f\"Average ROUGE-L score: {evaluation_results['avg_rougeL']:.4f}\")\n",
    "\n",
    "# Save results to a file\n",
    "with open(\"evaluation_results_cot.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation results saved to evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since zero-shot prompting got better results, I will add the conversation memory to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"{chat_history} \\nAnswer the question based only on the following context:\\n{context}, Just type Answer: <YOUR ANSWER>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Memory-Enabled Chain ---\n",
      "Initial Question: In what country is Normandy located?\n",
      "\n",
      "--- Current Chat History ---\n",
      "No chat history yet\n",
      "-------------------------\n",
      "\n",
      "Answer: France\n",
      "\n",
      "Follow-up Question 1: When were the Normans in Normandy?\n",
      "\n",
      "--- Current Chat History ---\n",
      "Human: In what country is Normandy located?\n",
      "Ai: Answer: France\n",
      "-------------------------\n",
      "\n",
      "Answer: The Normans gave their name to Normandy, a region in France, in the 10th and 11th centuries.\n",
      "\n",
      "Follow-up Question 2: From which countries did the Norse originate?\n",
      "\n",
      "--- Current Chat History ---\n",
      "Human: In what country is Normandy located?\n",
      "Ai: Answer: France\n",
      "Human: When were the Normans in Normandy?\n",
      "Ai: Answer: The Normans gave their name to Normandy, a region in France, in the 10th and 11th centuries.\n",
      "-------------------------\n",
      "\n",
      "Answer: Denmark, Iceland, and Norway.\n",
      "\n",
      "Follow-up Question 3: Who was the Norse leader?\n",
      "\n",
      "--- Current Chat History ---\n",
      "Human: In what country is Normandy located?\n",
      "Ai: Answer: France\n",
      "Human: When were the Normans in Normandy?\n",
      "Ai: Answer: The Normans gave their name to Normandy, a region in France, in the 10th and 11th centuries.\n",
      "Human: From which countries did the Norse originate?\n",
      "Ai: Answer: Denmark, Iceland, and Norway.\n",
      "-------------------------\n",
      "\n",
      "Answer: Rollo\n"
     ]
    }
   ],
   "source": [
    "# Create a memory-enabled chain for testing with a single question\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "memory_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": load_memory,\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def ask_with_memory(question):\n",
    "    server_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    chat_history = memory.load_memory_variables({})\n",
    "    print(\"\\n--- Current Chat History ---\")\n",
    "    if chat_history.get(\"chat_history\"):\n",
    "        for message in chat_history[\"chat_history\"]:\n",
    "            print(\n",
    "                f\"{message.type.capitalize()}: {message.content[:100]}...\"\n",
    "                if len(message.content) > 100\n",
    "                else f\"{message.type.capitalize()}: {message.content}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"No chat history yet\")\n",
    "    print(\"-------------------------\\n\")\n",
    "    \n",
    "    result = memory_chain.invoke(question)\n",
    "    memory.save_context({\"input\": question}, {\"output\": result})\n",
    "\n",
    "    # Kill the server when done\n",
    "    server_process.terminate()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test the memory-enabled chain with a single question\n",
    "print(\"\\n--- Testing Memory-Enabled Chain ---\")\n",
    "question = 'In what country is Normandy located?'\n",
    "print(\"Initial Question:\", question)\n",
    "\n",
    "print(ask_with_memory(question))\n",
    "\n",
    "# Test follow-up questions\n",
    "follow_up_questions = [\n",
    "    'When were the Normans in Normandy?',  \n",
    "    'From which countries did the Norse originate?',  \n",
    "    'Who was the Norse leader?',\n",
    "]\n",
    "\n",
    "for i, follow_up in enumerate(follow_up_questions, 1):\n",
    "    print(f\"\\nFollow-up Question {i}: {follow_up}\")\n",
    "    print(ask_with_memory(follow_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
